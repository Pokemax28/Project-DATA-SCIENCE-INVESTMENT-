{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction ‚Äì Contexte  du projet\n",
    "\n",
    "Dans un march√© immobilier en constante √©volution, prendre une d√©cision d‚Äôinvestissement √©clair√©e repose sur une compr√©hension fine des dynamiques locales : prix au m¬≤, attractivit√© des quartiers, proximit√© des services, ou encore potentiel locatif.\n",
    "Gr√¢ce aux donn√©es fonci√®res publiques (DVF) mises √† disposition par le gouvernement fran√ßais, il est aujourd‚Äôhui possible d‚Äôanalyser ces tendances de mani√®re pr√©cise et objective.\n",
    "Cette √©tude a pour objectif de fournir une analyse compl√®te du march√© immobilier afin d‚Äôidentifier les zones les plus attractives et rentables pour un investisseur souhaitant se positionner sur le locatif r√©sidentiel.\n",
    "Elle s‚Äôappuie sur une approche data-driven, combinant le traitement, la visualisation et l‚Äôinterpr√©tation des donn√©es immobili√®res issues des ventes r√©elles.\n",
    "\n",
    "## Objectif de l'√©tude \n",
    "\n",
    "L‚Äôobjectif est de proposer √† l‚Äôinvestisseur : \n",
    "Une vision claire du march√© immobilier dans plusieurs zones urbaines et p√©riurbaines fran√ßaises ;\n",
    "Une √©valuation de la rentabilit√© potentielle en fonction du prix d‚Äôachat, du type de bien et du profil locatif vis√© ;\n",
    "Des recommandations pr√©cises sur les zones √† privil√©gier pour maximiser la rentabilit√© et minimiser le risque locatif.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "# üë©‚Äçüíº Persona ‚Äì Sara\n",
    "\n",
    "Sara, 28 ans, souhaite r√©aliser son premier investissement locatif.  \n",
    "Active et pr√©voyante, elle d√©sire placer son √©pargne dans un projet immobilier **simple, rentable et durable**.  \n",
    "Son objectif est d‚Äôacqu√©rir un bien **sans travaux**, situ√© √† proximit√© d‚Äôune **grande m√©tropole**, afin de b√©n√©ficier d‚Äôune **forte demande locative** et de limiter les risques de vacance.\n",
    "\n",
    "Sa **motivation principale** est de r√©aliser un **investissement concret et s√©curis√©**, dans une zone o√π la demande locative est naturellement soutenue.  \n",
    "Elle privil√©gie la **stabilit√©** et la **rentabilit√© √† long terme**, plut√¥t que la sp√©culation √† court terme.\n",
    "\n",
    "---\n",
    "\n",
    "<div style=\"display:flex; align-items:flex-start; gap:25px; margin-top:15px;\">\n",
    "\n",
    "  <img src=\"avatar_Sara.jpeg\" alt=\"Avatar Sara\" width=\"170\" style=\"border-radius:50%; box-shadow: 0 0 10px rgba(0,0,0,0.25);\">\n",
    "\n",
    "  <div style=\"line-height:1.7;\">\n",
    "\n",
    "  ### üë© Informations personnelles\n",
    "  - **√Çge :** 28 ans  \n",
    "  - **Situation :** jeune cadre salari√©e dans le secteur tertiaire  \n",
    "  - **Localisation actuelle :** Lyon  \n",
    "  - **Statut :** primo-investisseuse  \n",
    "  - **Revenu mensuel net :** environ 2 500 ‚Ç¨  \n",
    "\n",
    "  ### üéØ Objectifs d‚Äôinvestissement\n",
    "  - R√©aliser un **premier placement locatif s√©curis√©**  \n",
    "  - Cr√©er une **source de revenus compl√©mentaires**  \n",
    "  - Se constituer un **patrimoine immobilier** sur le long terme  \n",
    "  - Trouver un bien **cl√© en main**, sans travaux ni gestion complexe  \n",
    "\n",
    "  ### üí∞ Budget et contraintes\n",
    "  - **Budget global :** 100 000 √† 150 000 ‚Ç¨ (frais inclus)  \n",
    "  - Aucun travaux √† pr√©voir (ni r√©novation, ni am√©nagement lourd)  \n",
    "  - Investissement √† cr√©dit avec un apport mod√©r√© (5 √† 10 %)  \n",
    "  - Rendement brut minimum recherch√© : **‚â• 5,5 %**\n",
    "\n",
    "  ### üèôÔ∏è Localisation et crit√®res de recherche\n",
    "  Sara cible prioritairement les **zones proches de grandes m√©tropoles** o√π la demande locative est forte :  \n",
    "  - Banlieues √©tudiantes, villes universitaires, quartiers desservis par les transports  \n",
    "  - Bonne accessibilit√©, commerces √† proximit√©, environnement s√©curis√©  \n",
    "\n",
    "  ### üè† Typologie de biens recherch√©s\n",
    "  **1. Investissement √©tudiant :**  \n",
    "  Studio ou T1 de 20 √† 30 m¬≤, situ√© √† moins de **20 minutes √† pied d‚Äôune universit√© ou d‚Äôune grande √©cole**, meubl√© ou pr√™t √† louer.  \n",
    "  Objectif : **rentabilit√© √©lev√©e**, avec un **turn-over locatif accept√©**.  \n",
    "\n",
    "  **2. Investissement jeune couple :**  \n",
    "  T2 ou petit T3 de 40 √† 55 m¬≤, avec **deux chambres**, dans un environnement calme et familial, proche des √©coles et services.  \n",
    "  Objectif : **stabilit√© locative sur le long terme**.  \n",
    "\n",
    "  ### üí¨ Sa demande\n",
    "  > ‚ÄúJe veux un bien qui se loue facilement, sans avoir √† g√©rer des travaux ou des impr√©vus.  \n",
    "  > L‚Äôid√©e, c‚Äôest de construire un patrimoine qui me rapporte d√®s aujourd‚Äôhui.‚Äù\n",
    "\n",
    "  </div>\n",
    "</div>\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import glob\n",
    "import os\n",
    "from sklearn.neighbors import BallTree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notebook configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', 100)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Collect\n",
    "\n",
    "Toutes les donn√©es viennent de ces liens.\n",
    "\n",
    "- **Loyers eu m2 pour T1 et T2** : [https://www.data.gouv.fr/datasets/carte-des-loyers-indicateurs-de-loyers-dannonce-par-commune-en-2023/](https://www.data.gouv.fr/datasets/carte-des-loyers-indicateurs-de-loyers-dannonce-par-commune-en-2022/)\n",
    "- **Arret de transport** : https://www.data.gouv.fr/datasets/arrets-de-transport-en-france/\n",
    "- **Point culturel** : https://www.data.gouv.fr/datasets/base-des-lieux-et-equipements-culturels-basilic/\n",
    "- **Etablissements d'enseignement des premier et second degr√©s** : https://data.education.gouv.fr/explore/dataset/fr-en-adresse-et-geolocalisation-etablissements-premier-et-second-degre/export/?disjunctive.numero_uai&disjunctive.code_departement&disjunctive.code_region&disjunctive.code_academie&disjunctive.nature_uai&disjunctive.nature_uai_libe&disjunctive.code_commune&disjunctive.libelle_departement&disjunctive.libelle_region&disjunctive.libelle_academie&disjunctive.secteur_prive_code_type_contrat&disjunctive.secteur_prive_libelle_type_contrat&disjunctive.code_ministere&disjunctive.libelle_ministere\n",
    "- **Valeurs fonci√®re** : https://www.data.gouv.fr/datasets/demandes-de-valeurs-foncieres-geolocalisees/\n",
    "- **Prix et indices de prix √† la consommation** (Carburant) : https://www.data.gouv.fr/datasets/prix-des-carburants-en-france-flux-instantane-v2-amelioree/\n",
    "- d√©tail des loyers et autres par arrondissement de paris\n",
    "- https://www.data.gouv.fr/datasets/communes-de-france-base-des-codes-postaux\n",
    "\n",
    "Elles sont r√©cup√©rables via le lien Google Drive suivant : https://drive.google.com/file/d/1YN-JBqsOTZcvSHNrJZWFY7XpdvYqjkUx/view?usp=sharing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunks = pd.read_csv('../Data/Carburant.csv', chunksize=1000000, low_memory=False, sep=\";\")\n",
    "\n",
    "chunk_list = []\n",
    "for chunk in chunks:\n",
    "    print(\"Processing a new chunk...\")\n",
    "    df1 = pd.DataFrame(chunk)\n",
    "    df1.columns = df1.columns.str.strip().str.lower().str.replace(' ', '_').str.replace('(', '').str.replace(')', '')\n",
    "    chunk_list.append(df1[['code_postal', 'prix_gazole', 'prix_sp95' , 'prix_e85', 'prix_gplc', 'prix_sp98' , 'prix_e10']])\n",
    "\n",
    "# Concatenate all chunks into a single DataFrame\n",
    "df = pd.concat(chunk_list, ignore_index=True)\n",
    "mean = df.groupby(\"code_postal\").mean().reset_index()\n",
    "mean['totmean'] = mean[['prix_gazole', 'prix_sp95' , 'prix_e85', 'prix_sp98' , 'prix_e10']].where(mean[['prix_gazole', 'prix_sp95' , 'prix_e85', 'prix_sp98' , 'prix_e10']] > 0).mean(axis=1)\n",
    "mean['code_postal'] = mean['code_postal'].apply(lambda row: str(row).zfill(5))\n",
    "\n",
    "# Display basic information about the dataset\n",
    "print(mean.info())\n",
    "print(mean.sort_values(by=\"prix_gazole\", ascending=False).head(10))\n",
    "mean.to_csv('../DataCleaned/Carburant_Cleaned.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "code_postal_ville = [\n",
    "    #Ile-de-France\n",
    "    \"75000\",\n",
    "    \"75001\", \"75002\", \"75003\", \"75004\", \"75005\", \"75006\", \"75007\", \"75008\", \"75009\", \"75010\",\n",
    "    \"75011\", \"75012\", \"75013\", \"75014\", \"75015\", \"75016\", \"75017\", \"75018\", \"75019\", \"75020\",\n",
    "    \"92100\", \"92110\", \"92120\", \"92130\", \"92140\", \"92150\", \"92160\", \"92170\", \"92190\",\n",
    "    \"92200\", \"92210\", \"92220\", \"92230\", \"92240\", \"92250\", \"92260\", \"92270\", \"92290\",\n",
    "    \"92300\", \"92310\", \"92320\", \"92330\", \"92340\", \"92350\", \"92360\", \"92370\", \"92380\", \"92390\",   \"92400\", \"92410\", \"92420\", \"92430\", \"92440\", \"92450\", \"92460\", \"92470\", \"92480\", \"92490\",\n",
    "    \"92500\", \"92510\", \"92520\", \"92530\", \"92540\", \"92550\", \"92560\", \"92570\", \"92580\", \"92590\",\n",
    "    \"92600\", \"92610\", \"92620\", \"92630\", \"92640\", \"92650\", \"92660\", \"92670\", \"92680\", \"92690\",\n",
    "    \"92700\", \"92710\", \"92720\", \"92730\", \"92740\", \"92750\", \"92760\", \"92770\", \"92780\", \"92790\",\n",
    "    \"92800\", \"92810\", \"92820\", \"92830\", \"92840\", \"92850\", \"92860\", \"92870\", \"92880\", \"92890\",\n",
    "    \"92900\", \"92910\", \"92920\", \"92930\", \"92940\", \"92950\", \"92960\", \"92970\", \"92980\", \"92990\",\n",
    "    \"93000\", \"93100\", \"93200\", \"93300\", \"93400\", \"93500\", \"93600\", \"93700\", \"93800\", \"93900\",\n",
    "    \"94000\", \"94100\", \"94200\", \"94300\", \"94400\", \"94500\", \"94600\", \"94700\", \"94800\", \"94900\",\n",
    "    \n",
    "    #Marseille\n",
    "    \"13000\", \"13001\", \"13002\", \"13003\", \"13004\", \"13005\", \"13006\", \"13007\", \"13008\", \"13009\",\n",
    "    \"13010\", \"13011\", \"13012\", \"13013\",\n",
    "    \n",
    "    #Aix-en-Provence\n",
    "    \"13080\", \"13100\", \"13190\", \"13290\", \"13540\", \"13590\", \"13700\", \"13800\", \"13990\",\n",
    "    \n",
    "    #Lyon\n",
    "    \"69000\", \"69001\", \"69002\", \"69003\", \"69004\", \"69005\", \"69006\", \"69007\", \"69008\", \"69009\",\n",
    "    \"69100\", \"69200\", \"69300\", \"69400\", \"69500\", \"69600\", \"69700\", \"69800\", \"69900\",\n",
    "    \n",
    "    #Lille\n",
    "    \"59000\", \"59100\", \"59200\", \"59300\", \"59400\", \"59500\", \"59600\", \"59700\", \"59800\", \"59900\",\n",
    "    \n",
    "    #Bordeaux\n",
    "    \"33000\", \"33100\", \"33200\", \"33300\", \"33400\", \"33500\", \"33600\", \"33700\", \"33800\", \"33900\",\n",
    "    \n",
    "    #Toulouse\n",
    "    \"31000\", \"31100\", \"31200\", \"31300\", \"31400\", \"31500\", \"31600\", \"31700\", \"31800\", \"31900\",\n",
    "    \n",
    "    #Nice\n",
    "    \"06000\", \"06100\", \"06200\", \"06300\", \"06400\", \"06500\", \"06600\", \"06700\", \"06800\", \"06900\",\n",
    "    \n",
    "    #Nantes\n",
    "    \"44000\", \"44100\", \"44200\", \"44300\", \"44400\", \"44500\", \"44600\", \"44700\", \"44800\", \"44900\",\n",
    "    \n",
    "    #Strasbourg\n",
    "    \"67000\", \"67100\", \"67200\", \"67300\", \"67400\", \"67500\", \"67600\", \"67700\", \"67800\", \"67900\",\n",
    "    \n",
    "    #Montpellier\n",
    "    \"34000\", \"34100\", \"34200\", \"34300\", \"34400\", \"34500\", \"34600\", \"34700\", \"34800\", \"34900\",\n",
    "    \n",
    "    #Rennes\n",
    "    \"35000\", \"35100\", \"35200\", \"35300\", \"35400\", \"35500\", \"35600\", \"35700\", \"35800\", \"35900\",\n",
    "    \n",
    "    #Grenoble\n",
    "    \"38000\", \"38100\", \"38200\", \"38300\", \"38400\", \"38500\", \"38600\", \"38700\", \"38800\", \"38900\",\n",
    "    \n",
    "    #Dijon\n",
    "    \"21000\", \"21100\", \"21200\", \"21300\", \"21400\", \"21500\", \"21600\", \"21700\", \"21800\", \"21900\",   \n",
    "    \n",
    "    #Angers\n",
    "    \"49000\", \"49100\", \"49200\", \"49300\", \"49400\", \"49500\", \"49600\", \"49700\", \"49800\", \"49900\",\n",
    "    \n",
    "    #Rennes\n",
    "    \"35000\", \"35100\", \"35200\", \"35300\", \"35400\", \"35500\", \"35600\", \"35700\", \"35800\", \"35900\",\n",
    "    \n",
    "    #Le Havre\n",
    "    \"76000\", \"76100\", \"76200\", \"76300\", \"76400\", \"76500\", \"76600\", \"76700\", \"76800\", \"76900\",\n",
    "    \n",
    "    #Saint-√âtienne\n",
    "    \"42000\", \"42100\", \"42200\", \"42300\", \"42400\", \"42500\", \"42600\", \"42700\", \"42800\", \"42900\",\n",
    "]\n",
    "\n",
    "# === 1) Chemins des fichiers ===\n",
    "communes_path = \"../Data/communes-france-2025.csv\"\n",
    "adj_path = \"../Data/communes_adjacentes_2022_toutes.csv\"\n",
    "\n",
    "# === 2) Charger les fichiers (en texte) ===\n",
    "communes = pd.read_csv(communes_path, dtype=str)\n",
    "adj = pd.read_csv(adj_path, dtype=str)\n",
    "\n",
    "# === 3) V√©rifier que les colonnes utiles existent ===\n",
    "# communes: code INSEE, nom de la commune, codes postaux\n",
    "if not all(c in communes.columns for c in [\"code_insee\", \"nom_standard\", \"codes_postaux\"]):\n",
    "    raise ValueError(\"Le fichier communes doit contenir: code_insee, nom_standard, codes_postaux\")\n",
    "\n",
    "# adjacences: code INSEE source, voisins INSEE\n",
    "if not all(c in adj.columns for c in [\"insee\", \"insee_voisins\"]):\n",
    "    raise ValueError(\"Le fichier adjacences doit contenir: insee, insee_voisins\")\n",
    "\n",
    "# === 4) Petite fonction pour d√©couper les listes (s√©parateur '|') ===\n",
    "def split_pipe(value):\n",
    "    \"\"\"Retourne une liste en s√©parant par '|' (ou liste vide si NaN).\"\"\"\n",
    "    if pd.isna(value):\n",
    "        return []\n",
    "    text = str(value).strip()\n",
    "    if text == \"\":\n",
    "        return []\n",
    "    return [x.strip() for x in text.split(\"|\") if x.strip()]\n",
    "\n",
    "# === 5) Pr√©parer la table 'communes' ===\n",
    "# On garde seulement les colonnes utiles, et on \"explose\" les codes postaux\n",
    "communes_simple = communes[[\"code_insee\", \"nom_standard\", \"codes_postaux\"]].copy()\n",
    "communes_simple[\"liste_cp\"] = communes_simple[\"codes_postaux\"].apply(split_pipe)\n",
    "communes_cp = communes_simple.explode(\"liste_cp\", ignore_index=True)  # une ligne par code postal\n",
    "communes_cp = communes_cp.rename(columns={\n",
    "    \"nom_standard\": \"commune\",\n",
    "    \"liste_cp\": \"code_postal\"\n",
    "})\n",
    "\n",
    "# === 6) Pr√©parer la table 'adjacences' ===\n",
    "# On \"explose\" les voisins INSEE (une ligne par voisin)\n",
    "adj[\"voisin_insee\"] = adj[\"insee_voisins\"].apply(split_pipe)\n",
    "adj_long = adj.explode(\"voisin_insee\", ignore_index=True)\n",
    "adj_long = adj_long.dropna(subset=[\"voisin_insee\"])  # garder seulement les lignes avec un voisin\n",
    "\n",
    "# === 7) Joindre pour r√©cup√©rer les infos de la banlieue (voisine) ===\n",
    "# On va chercher, pour chaque voisin_insee, son nom et ses codes postaux\n",
    "banlieue_infos = communes_simple.rename(columns={\n",
    "    \"code_insee\": \"code_insee_banlieue\",\n",
    "    \"nom_standard\": \"banlieue\",\n",
    "    \"codes_postaux\": \"codes_postaux_banlieue\"\n",
    "})\n",
    "\n",
    "adj_avec_banlieue = adj_long.merge(\n",
    "    banlieue_infos,\n",
    "    left_on=\"voisin_insee\",\n",
    "    right_on=\"code_insee_banlieue\",\n",
    "    how=\"left\"\n",
    ")\n",
    "\n",
    "# === 8) Joindre pour r√©cup√©rer les infos de la commune source + son code postal (explos√©) ===\n",
    "# On relie le code INSEE source (adj[\"insee\"]) au code INSEE des communes (communes_cp[\"code_insee\"])\n",
    "final = adj_avec_banlieue.merge(\n",
    "    communes_cp[[\"code_insee\", \"commune\", \"code_postal\"]],\n",
    "    left_on=\"insee\",\n",
    "    right_on=\"code_insee\",\n",
    "    how=\"left\"\n",
    ")\n",
    "\n",
    "# === 9) Ne garder que les colonnes finales, enlever doublons et lignes incompl√®tes ===\n",
    "banlieues_df = final[[\n",
    "    \"code_postal\",           # CP de la commune source\n",
    "    \"commune\",               # Nom de la commune source\n",
    "    \"insee\",                 # INSEE source\n",
    "    \"banlieue\",              # Nom de la commune voisine (banlieue)\n",
    "    \"code_insee_banlieue\",   # INSEE de la banlieue\n",
    "    \"codes_postaux_banlieue\" # CP(s) de la banlieue (s√©par√©s par '|')\n",
    "]].rename(columns={\"insee\": \"code_insee_source\"})\n",
    "\n",
    "# On enl√®ve les lignes o√π il manque l'essentiel\n",
    "banlieues_df = banlieues_df.dropna(subset=[\"code_postal\", \"commune\", \"banlieue\"])\n",
    "banlieues_df = banlieues_df.drop_duplicates()\n",
    "\n",
    "# === Filtrer une zone, ex. √éle-de-France (75, 92, 93, 94) ===, Paris, Marseille, \n",
    "banlieues_df = banlieues_df[banlieues_df[\"code_postal\"].str.match(r\"^(75|13|59|49|80|44|38)\")]\n",
    "\n",
    "# ---  Normaliser les codes postaux  ---\n",
    "banlieues_df[\"code_postal\"] = (\n",
    "    banlieues_df[\"code_postal\"]\n",
    "    .astype(str)\n",
    "    .str.extract(r\"(\\d{2,5})\", expand=False)  # r√©cup√®re les chiffres principaux\n",
    "    .fillna(\"\")\n",
    "    .str.zfill(5)\n",
    ")\n",
    "\n",
    "# ---  Filtrer : ne garder que les lignes dont le CP SOURCE est dans ta liste ---\n",
    "cp_set = set(code_postal_ville)\n",
    "extrait_source = banlieues_df[banlieues_df[\"code_postal\"].isin(cp_set)].copy()\n",
    "\n",
    "# ---  inclure aussi si la BANLIEUE poss√®de un CP dans ta liste ---\n",
    "def banlieue_a_cp_dans_liste(cell, cp_set):\n",
    "    \"\"\"Retourne True si au moins un CP (s√©par√©s par '|') de la banlieue est dans code_postal_ville.\"\"\"\n",
    "    if pd.isna(cell) or str(cell).strip() == \"\":\n",
    "        return False\n",
    "    return any(cp.strip().zfill(5) in cp_set for cp in str(cell).split(\"|\"))\n",
    "\n",
    "mask_banlieue = banlieues_df[\"codes_postaux_banlieue\"].apply(lambda x: banlieue_a_cp_dans_liste(x, cp_set))\n",
    "extrait_banlieue = banlieues_df[mask_banlieue].copy()\n",
    "code_postal_ville.extend(extrait_banlieue[\"code_postal\"].tolist())\n",
    "dfliste = pd.Series(code_postal_ville)\n",
    "\n",
    "outfile = \"../DataCleaned/metropoleetbanlieues_liste.csv\"\n",
    "dfliste.to_csv(outfile, index=False, encoding=\"utf-8\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunks = pd.read_csv('../Data/Culture.csv', chunksize=1000000, low_memory=False, sep=\";\")\n",
    "\n",
    "\n",
    "chunk_list = []\n",
    "for chunk in chunks:\n",
    "    print(\"Processing a new chunk...\")\n",
    "    df1 = pd.DataFrame(chunk)\n",
    "    df2 = df1[['Code_Postal']].dropna()\n",
    "    chunk_list.append(df2)\n",
    "\n",
    "# Concatenate all chunks into a single DataFrame\n",
    "df = pd.concat(chunk_list, ignore_index=True)\n",
    "\n",
    "# Display basic information about the dataset\n",
    "counts = df.groupby(\"Code_Postal\").size().reset_index(name=\"CultureCount\")\n",
    "print(counts.sort_values(by=\"CultureCount\", ascending=False).head(10))\n",
    "\n",
    "counts.to_csv('../DataCleaned/Culture_Cleaned.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the CSV\n",
    "chunks = pd.read_csv('../Data/ValeurFonciere.csv', chunksize=1000000, low_memory=False, sep=\",\")\n",
    "\n",
    "chunk_list = []\n",
    "for chunk in chunks:\n",
    "    print(\"Processing a new chunk...\")\n",
    "    df1 = pd.DataFrame(chunk)\n",
    "    df1 = df1[['nature_mutation','valeur_fonciere','code_postal','surface_reelle_bati','nombre_pieces_principales','type_local','longitude','latitude','date_mutation']]\n",
    "    df1 = df1.dropna(subset=['code_postal'])\n",
    "    df1['code_postal'] = df1['code_postal'].astype(float).astype(int).astype(str).str.zfill(5)\n",
    "    df1['date_mutation'] = pd.to_datetime(df1['date_mutation'])\n",
    "    chunk_list.append(df1)\n",
    "\n",
    "# Concatenate all chunks into a single DataFrame\n",
    "dftot = pd.concat(chunk_list, ignore_index=True)\n",
    "\n",
    "# Display basic information about the dataset\n",
    "print(dftot.info())\n",
    "print(dftot.head())\n",
    "\n",
    "dftot.to_csv('../DataCleaned/DVF_Cleaned.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the CSV\n",
    "chunks = pd.read_csv('../Data/Education.csv', chunksize=1000000, low_memory=False, sep=\";\")\n",
    "\n",
    "chunk_list = []\n",
    "for chunk in chunks:\n",
    "    print(\"Processing a new chunk...\")\n",
    "    df1 = pd.DataFrame(chunk)\n",
    "    df2 = df1[['Adresse : code postal']].dropna().copy()\n",
    "    df2 = df2.rename(columns={'Adresse : code postal':'CodePostal'})\n",
    "    df2['CodePostal'] = df2['CodePostal'].apply(lambda value: str(value).zfill(5))\n",
    "    chunk_list.append(df2)\n",
    "\n",
    "# Concatenate all chunks into a single DataFrame\n",
    "df = pd.concat(chunk_list, ignore_index=True)\n",
    "count = df.groupby(\"CodePostal\").size().reset_index(name=\"EducationCount\")\n",
    "\n",
    "# Display basic information about the dataset\n",
    "print(count.info())\n",
    "print(count.head())\n",
    "print(count.sort_values(by=\"EducationCount\", ascending=False).head(10))\n",
    "count.to_csv('../DataCleaned/EducationCount_Cleaned.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the CSV\n",
    "chunks = pd.read_csv('../Data/LoyerT1T2.csv', chunksize=1000000, low_memory=False, sep=\";\", encoding=\"latin1\")\n",
    "\n",
    "chunk_list = []\n",
    "for chunk in chunks:\n",
    "    print(\"Processing a new chunk...\")\n",
    "    df1 = pd.DataFrame(chunk)\n",
    "    df2 = df1[['DEP', 'loypredm2']].dropna().copy()\n",
    "    df2 = df2.rename(columns={'DEP':'D√©partement','loypredm2':'Loyerm2'})\n",
    "    df2['Loyerm2'] = df2['Loyerm2'].astype(str).str.replace(',', '.').astype(float)\n",
    "    chunk_list.append(df2)\n",
    "\n",
    "# Concatenate all chunks into a single DataFrame\n",
    "df = pd.concat(chunk_list, ignore_index=True)\n",
    "df['Loyerm2'] = pd.to_numeric(df['Loyerm2'], errors='coerce')\n",
    "mean = df.groupby(\"D√©partement\")[\"Loyerm2\"].mean().reset_index(name=\"Loyerm2Mean\")\n",
    "\n",
    "# Display basic information about the dataset\n",
    "print(mean.sort_values(by=\"Loyerm2Mean\", ascending=False).head())\n",
    "mean.to_csv('../DataCleaned/LoyerT1T2_Cleaned.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stop_lat,stop_lon\n",
    "\n",
    "# Load the CSV\n",
    "df = pd.read_csv(\"../Data/latlongCP.csv\", sep=\",\",low_memory=False)\n",
    "chunks = pd.read_csv('../Data/Transport.csv', chunksize=1000000, low_memory=False, sep=\",\")\n",
    "\n",
    "# Extract lat/lon as radians (for haversine distance)\n",
    "df[\"lat\"] = df[\"latitude\"].astype(float)\n",
    "df[\"lon\"] = df[\"longitude\"].astype(float)\n",
    "df = df.dropna(subset=[\"lat\", \"lon\", \"code_postal\"])\n",
    "df = df.dropna()\n",
    "coords_rad = np.radians(df[[\"lat\", \"lon\"]].to_numpy())\n",
    "\n",
    "# Build BallTree (fast nearest-neighbor search)\n",
    "tree = BallTree(coords_rad, metric=\"haversine\")\n",
    "\n",
    "def latlon_to_zip(lat, lon):\n",
    "    \"\"\"Return closest postal code for given latitude, longitude.\"\"\"\n",
    "    dist, idx = tree.query(np.radians([[lat, lon]]), k=1)\n",
    "    postal = df.iloc[idx[0][0]][\"code_postal\"]\n",
    "    postal = str(postal).zfill(5)\n",
    "    return str(postal)\n",
    "\n",
    "chunk_list = []\n",
    "for chunk in chunks:\n",
    "    print(\"Processing a new chunk...\")\n",
    "    df1 = pd.DataFrame(chunk)\n",
    "    df2 = df1[['stop_lat','stop_lon']].dropna()\n",
    "    df3 = pd.DataFrame()\n",
    "    df3['CodePostal'] = df2.apply(lambda row: latlon_to_zip(row['stop_lat'], row['stop_lon']), axis=1)\n",
    "    chunk_list.append(df3)\n",
    "\n",
    "# Concatenate all chunks into a single DataFrame\n",
    "df = pd.concat(chunk_list, ignore_index=True)\n",
    "\n",
    "# Display basic information about the dataset\n",
    "counts = df3.groupby(\"CodePostal\").size().reset_index(name=\"TransportCount\")\n",
    "print(counts.sort_values(by=\"TransportCount\", ascending=False).head(10))\n",
    "\n",
    "counts.to_csv('../DataCleaned/TransportCount_Cleaned.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Les donn√©es sont nettoy√©es.\n",
    "Un version de tous chaque dataset est dissponible via le lien Google Drive suivant : https://drive.google.com/file/d/1l78W2CqDkuUzNHIQrd7voMC0O00ExEa7/view?usp=sharing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data join"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pd' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m df_all_temp = {}\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m dataset \u001b[38;5;129;01min\u001b[39;00m [\u001b[33m'\u001b[39m\u001b[33mCulture_Cleaned\u001b[39m\u001b[33m'\u001b[39m,\u001b[33m'\u001b[39m\u001b[33mCarburant_Cleaned\u001b[39m\u001b[33m'\u001b[39m,\u001b[33m'\u001b[39m\u001b[33mEducationCount_Cleaned\u001b[39m\u001b[33m'\u001b[39m,\u001b[33m'\u001b[39m\u001b[33mTransportCount_Cleaned\u001b[39m\u001b[33m'\u001b[39m,\u001b[33m'\u001b[39m\u001b[33mLoyerT1T2_Cleaned\u001b[39m\u001b[33m'\u001b[39m,\u001b[33m'\u001b[39m\u001b[33mmetropoleetbanlieues_liste\u001b[39m\u001b[33m'\u001b[39m,\u001b[33m'\u001b[39m\u001b[33mDVF_Cleaned\u001b[39m\u001b[33m'\u001b[39m]:\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m     df_all_temp[dataset] = \u001b[43mpd\u001b[49m.read_csv(\u001b[33m'\u001b[39m\u001b[33m../DataCleaned/\u001b[39m\u001b[33m'\u001b[39m+dataset+\u001b[33m'\u001b[39m\u001b[33m.csv\u001b[39m\u001b[33m'\u001b[39m, low_memory=\u001b[38;5;28;01mFalse\u001b[39;00m, sep=\u001b[33m\"\u001b[39m\u001b[33m,\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      5\u001b[39m df_all = df_all_temp[\u001b[33m'\u001b[39m\u001b[33mCulture_Cleaned\u001b[39m\u001b[33m'\u001b[39m].merge(df_all_temp[\u001b[33m'\u001b[39m\u001b[33mCarburant_Cleaned\u001b[39m\u001b[33m'\u001b[39m], left_on=\u001b[33m\"\u001b[39m\u001b[33mCodePostal\u001b[39m\u001b[33m\"\u001b[39m, right_on=\u001b[33m\"\u001b[39m\u001b[33mcode_postal\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      6\u001b[39m df_all_temp.pop(\u001b[33m'\u001b[39m\u001b[33mCulture_Cleaned\u001b[39m\u001b[33m'\u001b[39m)\n",
      "\u001b[31mNameError\u001b[39m: name 'pd' is not defined"
     ]
    }
   ],
   "source": [
    "df_all_temp = {}\n",
    "for dataset in ['Culture_Cleaned','Carburant_Cleaned','EducationCount_Cleaned','TransportCount_Cleaned','LoyerT1T2_Cleaned','metropoleetbanlieues_liste','DVF_Cleaned']:\n",
    "    df_all_temp[dataset] = pd.read_csv('../DataCleaned/'+dataset+'.csv', low_memory=False, sep=\",\")\n",
    "\n",
    "df_all = df_all_temp['DVF_Cleaned'].merge(df_all_temp['Carburant_Cleaned'], left_on=\"code_postal\", right_on=\"code_postal\")\n",
    "df_all_temp.pop('DVF_Cleaned')\n",
    "df_all_temp.pop('Carburant_Cleaned')\n",
    "df_all = df_all.merge(df_all_temp['metropoleetbanlieues_liste'], left_on=\"code_postal\", right_on=\"0\", how='left')\n",
    "df_all_temp.pop('metropoleetbanlieues_liste')\n",
    "df_all = df_all.merge(df_all_temp['EducationCount_Cleaned'], left_on=\"code_postal\", right_on=\"CodePostal\", how='left')\n",
    "df_all_temp.pop('EducationCount_Cleaned')\n",
    "df_all = df_all.merge(df_all_temp['TransportCount_Cleaned'], left_on=\"code_postal\", right_on=\"CodePostal\", how='left')\n",
    "df_all_temp.pop('TransportCount_Cleaned')\n",
    "df_all['D√©partement'] = df_all['code_postal'].apply(lambda x : str(x).zfill(5)).str[:2]\n",
    "df_all = df_all.merge(df_all_temp['LoyerT1T2_Cleaned'], left_on=\"D√©partement\", right_on=\"D√©partement\", how='left')\n",
    "df_all_temp.pop('LoyerT1T2_Cleaned')\n",
    "df_all['CodePostal'] = df_all['code_postal'].apply(lambda x : str(x).zfill(5))\n",
    "df_all = df_all.merge(df_all_temp['Culture_Cleaned'], left_on=\"CodePostal\", right_on=\"Code_Postal\", how='left')\n",
    "df_all_temp.pop('Culture_Cleaned')\n",
    "\n",
    "print(df_all.head())\n",
    "\n",
    "df_all.to_csv('../DataAll/Dataset.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Une fois les donn√©es rassembl√©e, nous pouvons les utiliser pour faire l'analyse.\n",
    "Une version de ce dataset join est disponible via le lien Google Drive suivant : https://drive.google.com/file/d/1XiIpsO4sZ9qwRYCcc6EtoccFavcKQEwE/view?usp=sharing"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venvMacOS",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
