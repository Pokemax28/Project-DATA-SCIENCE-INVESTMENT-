{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction ‚Äì Contexte  du projet\n",
    "\n",
    "Dans un march√© immobilier en constante √©volution, prendre une d√©cision d‚Äôinvestissement √©clair√©e repose sur une compr√©hension fine des dynamiques locales : prix au m¬≤, attractivit√© des quartiers, proximit√© des services, ou encore potentiel locatif.\n",
    "Gr√¢ce aux donn√©es fonci√®res publiques (DVF) mises √† disposition par le gouvernement fran√ßais, il est aujourd‚Äôhui possible d‚Äôanalyser ces tendances de mani√®re pr√©cise et objective.\n",
    "Cette √©tude a pour objectif de fournir une analyse compl√®te du march√© immobilier afin d‚Äôidentifier les zones les plus attractives et rentables pour un investisseur souhaitant se positionner sur le locatif r√©sidentiel.\n",
    "Elle s‚Äôappuie sur une approche data-driven, combinant le traitement, la visualisation et l‚Äôinterpr√©tation des donn√©es immobili√®res issues des ventes r√©elles.\n",
    "\n",
    "## Objectif de l'√©tude \n",
    "\n",
    "L‚Äôobjectif est de proposer √† l‚Äôinvestisseur : \n",
    "Une vision claire du march√© immobilier dans plusieurs zones urbaines et p√©riurbaines fran√ßaises ;\n",
    "Une √©valuation de la rentabilit√© potentielle en fonction du prix d‚Äôachat, du type de bien et du profil locatif vis√© ;\n",
    "Des recommandations pr√©cises sur les zones √† privil√©gier pour maximiser la rentabilit√© et minimiser le risque locatif.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "# üë©‚Äçüíº Persona ‚Äì Sara\n",
    "\n",
    "Sara, 28 ans, souhaite r√©aliser son premier investissement locatif.  \n",
    "Active et pr√©voyante, elle d√©sire placer son √©pargne dans un projet immobilier **simple, rentable et durable**.  \n",
    "Son objectif est d‚Äôacqu√©rir un bien **sans travaux**, situ√© √† proximit√© d‚Äôune **grande m√©tropole**, afin de b√©n√©ficier d‚Äôune **forte demande locative** et de limiter les risques de vacance.\n",
    "\n",
    "Sa **motivation principale** est de r√©aliser un **investissement concret et s√©curis√©**, dans une zone o√π la demande locative est naturellement soutenue.  \n",
    "Elle privil√©gie la **stabilit√©** et la **rentabilit√© √† long terme**, plut√¥t que la sp√©culation √† court terme.\n",
    "\n",
    "---\n",
    "\n",
    "<div style=\"display:flex; align-items:flex-start; gap:25px; margin-top:15px;\">\n",
    "\n",
    "  <img src=\"avatar_Sara.jpeg\" alt=\"Avatar Sara\" width=\"170\" style=\"border-radius:50%; box-shadow: 0 0 10px rgba(0,0,0,0.25);\">\n",
    "\n",
    "  <div style=\"line-height:1.7;\">\n",
    "\n",
    "  ### üë© Informations personnelles\n",
    "  - **√Çge :** 28 ans  \n",
    "  - **Situation :** jeune cadre salari√©e dans le secteur tertiaire  \n",
    "  - **Localisation actuelle :** Lyon  \n",
    "  - **Statut :** primo-investisseuse  \n",
    "  - **Revenu mensuel net :** environ 2 500 ‚Ç¨  \n",
    "\n",
    "  ### üéØ Objectifs d‚Äôinvestissement\n",
    "  - R√©aliser un **premier placement locatif s√©curis√©**  \n",
    "  - Cr√©er une **source de revenus compl√©mentaires**  \n",
    "  - Se constituer un **patrimoine immobilier** sur le long terme  \n",
    "  - Trouver un bien **cl√© en main**, sans travaux ni gestion complexe  \n",
    "\n",
    "  ### üí∞ Budget et contraintes\n",
    "  - **Budget global :** 100 000 √† 150 000 ‚Ç¨ (frais inclus)  \n",
    "  - Aucun travaux √† pr√©voir (ni r√©novation, ni am√©nagement lourd)  \n",
    "  - Investissement √† cr√©dit avec un apport mod√©r√© (5 √† 10 %)  \n",
    "  - Rendement brut minimum recherch√© : **‚â• 5,5 %**\n",
    "\n",
    "  ### üèôÔ∏è Localisation et crit√®res de recherche\n",
    "  Sara cible prioritairement les **zones proches de grandes m√©tropoles** o√π la demande locative est forte :  \n",
    "  - Banlieues √©tudiantes, villes universitaires, quartiers desservis par les transports  \n",
    "  - Bonne accessibilit√©, commerces √† proximit√©, environnement s√©curis√©  \n",
    "\n",
    "  ### üè† Typologie de biens recherch√©s\n",
    "  **1. Investissement √©tudiant :**  \n",
    "  Studio ou T1 de 20 √† 30 m¬≤, situ√© √† moins de **20 minutes √† pied d‚Äôune universit√© ou d‚Äôune grande √©cole**, meubl√© ou pr√™t √† louer.  \n",
    "  Objectif : **rentabilit√© √©lev√©e**, avec un **turn-over locatif accept√©**.  \n",
    "\n",
    "  **2. Investissement jeune couple :**  \n",
    "  T2 ou petit T3 de 40 √† 55 m¬≤, avec **deux chambres**, dans un environnement calme et familial, proche des √©coles et services.  \n",
    "  Objectif : **stabilit√© locative sur le long terme**.  \n",
    "\n",
    "  ### üí¨ Sa demande\n",
    "  > ‚ÄúJe veux un bien qui se loue facilement, sans avoir √† g√©rer des travaux ou des impr√©vus.  \n",
    "  > L‚Äôid√©e, c‚Äôest de construire un patrimoine qui me rapporte d√®s aujourd‚Äôhui.‚Äù\n",
    "\n",
    "  </div>\n",
    "</div>\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import glob\n",
    "import os\n",
    "from sklearn.neighbors import BallTree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notebook configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', 100)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Collect\n",
    "\n",
    "Toutes les donn√©es viennent de ces liens.\n",
    "\n",
    "- **Loyers eu m2 pour T1 et T2** : [https://www.data.gouv.fr/datasets/carte-des-loyers-indicateurs-de-loyers-dannonce-par-commune-en-2023/](https://www.data.gouv.fr/datasets/carte-des-loyers-indicateurs-de-loyers-dannonce-par-commune-en-2022/)\n",
    "- **Arret de transport** : https://www.data.gouv.fr/datasets/arrets-de-transport-en-france/\n",
    "- **Point culturel** : https://www.data.gouv.fr/datasets/base-des-lieux-et-equipements-culturels-basilic/\n",
    "- **Etablissements d'enseignement des premier et second degr√©s** : https://data.education.gouv.fr/explore/dataset/fr-en-adresse-et-geolocalisation-etablissements-premier-et-second-degre/export/?disjunctive.numero_uai&disjunctive.code_departement&disjunctive.code_region&disjunctive.code_academie&disjunctive.nature_uai&disjunctive.nature_uai_libe&disjunctive.code_commune&disjunctive.libelle_departement&disjunctive.libelle_region&disjunctive.libelle_academie&disjunctive.secteur_prive_code_type_contrat&disjunctive.secteur_prive_libelle_type_contrat&disjunctive.code_ministere&disjunctive.libelle_ministere\n",
    "- **Valeurs fonci√®re** : https://www.data.gouv.fr/datasets/demandes-de-valeurs-foncieres-geolocalisees/\n",
    "- **Prix et indices de prix √† la consommation** (Carburant) : https://www.data.gouv.fr/datasets/prix-des-carburants-en-france-flux-instantane-v2-amelioree/\n",
    "- d√©tail des loyers et autres par arrondissement de paris\n",
    "- https://www.data.gouv.fr/datasets/communes-de-france-base-des-codes-postaux\n",
    "\n",
    "Elles sont r√©cup√©rables via le lien Google Drive suivant : https://drive.google.com/file/d/1YN-JBqsOTZcvSHNrJZWFY7XpdvYqjkUx/view?usp=sharing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunks = pd.read_csv('../Data/Carburant.csv', chunksize=1000000, low_memory=False, sep=\";\")\n",
    "\n",
    "chunk_list = []\n",
    "for chunk in chunks:\n",
    "    print(\"Processing a new chunk...\")\n",
    "    df1 = pd.DataFrame(chunk)\n",
    "    df1.columns = df1.columns.str.strip().str.lower().str.replace(' ', '_').str.replace('(', '').str.replace(')', '')\n",
    "    chunk_list.append(df1[['code_postal', 'prix_gazole', 'prix_sp95' , 'prix_e85', 'prix_gplc', 'prix_sp98' , 'prix_e10']])\n",
    "\n",
    "# Concatenate all chunks into a single DataFrame\n",
    "df = pd.concat(chunk_list, ignore_index=True)\n",
    "mean = df.groupby(\"code_postal\").mean().reset_index()\n",
    "mean['totmean'] = mean[['prix_gazole', 'prix_sp95' , 'prix_e85', 'prix_sp98' , 'prix_e10']].where(mean[['prix_gazole', 'prix_sp95' , 'prix_e85', 'prix_sp98' , 'prix_e10']] > 0).mean(axis=1)\n",
    "mean['code_postal'] = mean['code_postal'].apply(lambda row: str(row).zfill(5))\n",
    "\n",
    "# Display basic information about the dataset\n",
    "print(mean.info())\n",
    "print(mean.sort_values(by=\"prix_gazole\", ascending=False).head(10))\n",
    "mean.to_csv('../DataCleaned/Carburant_Cleaned.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "code_postal_ville = [\n",
    "    #Ile-de-France\n",
    "    \"75000\",\n",
    "    \"75001\", \"75002\", \"75003\", \"75004\", \"75005\", \"75006\", \"75007\", \"75008\", \"75009\", \"75010\",\n",
    "    \"75011\", \"75012\", \"75013\", \"75014\", \"75015\", \"75016\", \"75017\", \"75018\", \"75019\", \"75020\",\n",
    "    \"92100\", \"92110\", \"92120\", \"92130\", \"92140\", \"92150\", \"92160\", \"92170\", \"92190\",\n",
    "    \"92200\", \"92210\", \"92220\", \"92230\", \"92240\", \"92250\", \"92260\", \"92270\", \"92290\",\n",
    "    \"92300\", \"92310\", \"92320\", \"92330\", \"92340\", \"92350\", \"92360\", \"92370\", \"92380\", \"92390\",   \"92400\", \"92410\", \"92420\", \"92430\", \"92440\", \"92450\", \"92460\", \"92470\", \"92480\", \"92490\",\n",
    "    \"92500\", \"92510\", \"92520\", \"92530\", \"92540\", \"92550\", \"92560\", \"92570\", \"92580\", \"92590\",\n",
    "    \"92600\", \"92610\", \"92620\", \"92630\", \"92640\", \"92650\", \"92660\", \"92670\", \"92680\", \"92690\",\n",
    "    \"92700\", \"92710\", \"92720\", \"92730\", \"92740\", \"92750\", \"92760\", \"92770\", \"92780\", \"92790\",\n",
    "    \"92800\", \"92810\", \"92820\", \"92830\", \"92840\", \"92850\", \"92860\", \"92870\", \"92880\", \"92890\",\n",
    "    \"92900\", \"92910\", \"92920\", \"92930\", \"92940\", \"92950\", \"92960\", \"92970\", \"92980\", \"92990\",\n",
    "    \"93000\", \"93100\", \"93200\", \"93300\", \"93400\", \"93500\", \"93600\", \"93700\", \"93800\", \"93900\",\n",
    "    \"94000\", \"94100\", \"94200\", \"94300\", \"94400\", \"94500\", \"94600\", \"94700\", \"94800\", \"94900\",\n",
    "    \n",
    "    #Marseille\n",
    "    \"13000\", \"13001\", \"13002\", \"13003\", \"13004\", \"13005\", \"13006\", \"13007\", \"13008\", \"13009\",\n",
    "    \"13010\", \"13011\", \"13012\", \"13013\",\n",
    "    \n",
    "    #Aix-en-Provence\n",
    "    \"13080\", \"13100\", \"13190\", \"13290\", \"13540\", \"13590\", \"13700\", \"13800\", \"13990\",\n",
    "    \n",
    "    #Lyon\n",
    "    \"69000\", \"69001\", \"69002\", \"69003\", \"69004\", \"69005\", \"69006\", \"69007\", \"69008\", \"69009\",\n",
    "    \"69100\", \"69200\", \"69300\", \"69400\", \"69500\", \"69600\", \"69700\", \"69800\", \"69900\",\n",
    "    \n",
    "    #Lille\n",
    "    \"59000\", \"59100\", \"59200\", \"59300\", \"59400\", \"59500\", \"59600\", \"59700\", \"59800\", \"59900\",\n",
    "    \n",
    "    #Bordeaux\n",
    "    \"33000\", \"33100\", \"33200\", \"33300\", \"33400\", \"33500\", \"33600\", \"33700\", \"33800\", \"33900\",\n",
    "    \n",
    "    #Toulouse\n",
    "    \"31000\", \"31100\", \"31200\", \"31300\", \"31400\", \"31500\", \"31600\", \"31700\", \"31800\", \"31900\",\n",
    "    \n",
    "    #Nice\n",
    "    \"06000\", \"06100\", \"06200\", \"06300\", \"06400\", \"06500\", \"06600\", \"06700\", \"06800\", \"06900\",\n",
    "    \n",
    "    #Nantes\n",
    "    \"44000\", \"44100\", \"44200\", \"44300\", \"44400\", \"44500\", \"44600\", \"44700\", \"44800\", \"44900\",\n",
    "    \n",
    "    #Strasbourg\n",
    "    \"67000\", \"67100\", \"67200\", \"67300\", \"67400\", \"67500\", \"67600\", \"67700\", \"67800\", \"67900\",\n",
    "    \n",
    "    #Montpellier\n",
    "    \"34000\", \"34100\", \"34200\", \"34300\", \"34400\", \"34500\", \"34600\", \"34700\", \"34800\", \"34900\",\n",
    "    \n",
    "    #Rennes\n",
    "    \"35000\", \"35100\", \"35200\", \"35300\", \"35400\", \"35500\", \"35600\", \"35700\", \"35800\", \"35900\",\n",
    "    \n",
    "    #Grenoble\n",
    "    \"38000\", \"38100\", \"38200\", \"38300\", \"38400\", \"38500\", \"38600\", \"38700\", \"38800\", \"38900\",\n",
    "    \n",
    "    #Dijon\n",
    "    \"21000\", \"21100\", \"21200\", \"21300\", \"21400\", \"21500\", \"21600\", \"21700\", \"21800\", \"21900\",   \n",
    "    \n",
    "    #Angers\n",
    "    \"49000\", \"49100\", \"49200\", \"49300\", \"49400\", \"49500\", \"49600\", \"49700\", \"49800\", \"49900\",\n",
    "    \n",
    "    #Rennes\n",
    "    \"35000\", \"35100\", \"35200\", \"35300\", \"35400\", \"35500\", \"35600\", \"35700\", \"35800\", \"35900\",\n",
    "    \n",
    "    #Le Havre\n",
    "    \"76000\", \"76100\", \"76200\", \"76300\", \"76400\", \"76500\", \"76600\", \"76700\", \"76800\", \"76900\",\n",
    "    \n",
    "    #Saint-√âtienne\n",
    "    \"42000\", \"42100\", \"42200\", \"42300\", \"42400\", \"42500\", \"42600\", \"42700\", \"42800\", \"42900\",\n",
    "]\n",
    "\n",
    "# === 1) Chemins des fichiers ===\n",
    "communes_path = \"../Data/communes-france-2025.csv\"\n",
    "adj_path = \"../Data/communes_adjacentes_2022_toutes.csv\"\n",
    "\n",
    "# === 2) Charger les fichiers (en texte) ===\n",
    "communes = pd.read_csv(communes_path, dtype=str)\n",
    "adj = pd.read_csv(adj_path, dtype=str)\n",
    "\n",
    "# === 3) V√©rifier que les colonnes utiles existent ===\n",
    "# communes: code INSEE, nom de la commune, codes postaux\n",
    "if not all(c in communes.columns for c in [\"code_insee\", \"nom_standard\", \"codes_postaux\"]):\n",
    "    raise ValueError(\"Le fichier communes doit contenir: code_insee, nom_standard, codes_postaux\")\n",
    "\n",
    "# adjacences: code INSEE source, voisins INSEE\n",
    "if not all(c in adj.columns for c in [\"insee\", \"insee_voisins\"]):\n",
    "    raise ValueError(\"Le fichier adjacences doit contenir: insee, insee_voisins\")\n",
    "\n",
    "# === 4) Petite fonction pour d√©couper les listes (s√©parateur '|') ===\n",
    "def split_pipe(value):\n",
    "    \"\"\"Retourne une liste en s√©parant par '|' (ou liste vide si NaN).\"\"\"\n",
    "    if pd.isna(value):\n",
    "        return []\n",
    "    text = str(value).strip()\n",
    "    if text == \"\":\n",
    "        return []\n",
    "    return [x.strip() for x in text.split(\"|\") if x.strip()]\n",
    "\n",
    "# === 5) Pr√©parer la table 'communes' ===\n",
    "# On garde seulement les colonnes utiles, et on \"explose\" les codes postaux\n",
    "communes_simple = communes[[\"code_insee\", \"nom_standard\", \"codes_postaux\"]].copy()\n",
    "communes_simple[\"liste_cp\"] = communes_simple[\"codes_postaux\"].apply(split_pipe)\n",
    "communes_cp = communes_simple.explode(\"liste_cp\", ignore_index=True)  # une ligne par code postal\n",
    "communes_cp = communes_cp.rename(columns={\n",
    "    \"nom_standard\": \"commune\",\n",
    "    \"liste_cp\": \"code_postal\"\n",
    "})\n",
    "\n",
    "# === 6) Pr√©parer la table 'adjacences' ===\n",
    "# On \"explose\" les voisins INSEE (une ligne par voisin)\n",
    "adj[\"voisin_insee\"] = adj[\"insee_voisins\"].apply(split_pipe)\n",
    "adj_long = adj.explode(\"voisin_insee\", ignore_index=True)\n",
    "adj_long = adj_long.dropna(subset=[\"voisin_insee\"])  # garder seulement les lignes avec un voisin\n",
    "\n",
    "# === 7) Joindre pour r√©cup√©rer les infos de la banlieue (voisine) ===\n",
    "# On va chercher, pour chaque voisin_insee, son nom et ses codes postaux\n",
    "banlieue_infos = communes_simple.rename(columns={\n",
    "    \"code_insee\": \"code_insee_banlieue\",\n",
    "    \"nom_standard\": \"banlieue\",\n",
    "    \"codes_postaux\": \"codes_postaux_banlieue\"\n",
    "})\n",
    "\n",
    "adj_avec_banlieue = adj_long.merge(\n",
    "    banlieue_infos,\n",
    "    left_on=\"voisin_insee\",\n",
    "    right_on=\"code_insee_banlieue\",\n",
    "    how=\"left\"\n",
    ")\n",
    "\n",
    "# === 8) Joindre pour r√©cup√©rer les infos de la commune source + son code postal (explos√©) ===\n",
    "# On relie le code INSEE source (adj[\"insee\"]) au code INSEE des communes (communes_cp[\"code_insee\"])\n",
    "final = adj_avec_banlieue.merge(\n",
    "    communes_cp[[\"code_insee\", \"commune\", \"code_postal\"]],\n",
    "    left_on=\"insee\",\n",
    "    right_on=\"code_insee\",\n",
    "    how=\"left\"\n",
    ")\n",
    "\n",
    "# === 9) Ne garder que les colonnes finales, enlever doublons et lignes incompl√®tes ===\n",
    "banlieues_df = final[[\n",
    "    \"code_postal\",           # CP de la commune source\n",
    "    \"commune\",               # Nom de la commune source\n",
    "    \"insee\",                 # INSEE source\n",
    "    \"banlieue\",              # Nom de la commune voisine (banlieue)\n",
    "    \"code_insee_banlieue\",   # INSEE de la banlieue\n",
    "    \"codes_postaux_banlieue\" # CP(s) de la banlieue (s√©par√©s par '|')\n",
    "]].rename(columns={\"insee\": \"code_insee_source\"})\n",
    "\n",
    "# On enl√®ve les lignes o√π il manque l'essentiel\n",
    "banlieues_df = banlieues_df.dropna(subset=[\"code_postal\", \"commune\", \"banlieue\"])\n",
    "banlieues_df = banlieues_df.drop_duplicates()\n",
    "\n",
    "# === Filtrer une zone, ex. √éle-de-France (75, 92, 93, 94) ===, Paris, Marseille, \n",
    "banlieues_df = banlieues_df[banlieues_df[\"code_postal\"].str.match(r\"^(75|13|59|49|80|44|38)\")]\n",
    "\n",
    "# ---  Normaliser les codes postaux  ---\n",
    "banlieues_df[\"code_postal\"] = (\n",
    "    banlieues_df[\"code_postal\"]\n",
    "    .astype(str)\n",
    "    .str.extract(r\"(\\d{2,5})\", expand=False)  # r√©cup√®re les chiffres principaux\n",
    "    .fillna(\"\")\n",
    "    .str.zfill(5)\n",
    ")\n",
    "\n",
    "# ---  Filtrer : ne garder que les lignes dont le CP SOURCE est dans ta liste ---\n",
    "cp_set = set(code_postal_ville)\n",
    "extrait_source = banlieues_df[banlieues_df[\"code_postal\"].isin(cp_set)].copy()\n",
    "\n",
    "# ---  inclure aussi si la BANLIEUE poss√®de un CP dans ta liste ---\n",
    "def banlieue_a_cp_dans_liste(cell, cp_set):\n",
    "    \"\"\"Retourne True si au moins un CP (s√©par√©s par '|') de la banlieue est dans code_postal_ville.\"\"\"\n",
    "    if pd.isna(cell) or str(cell).strip() == \"\":\n",
    "        return False\n",
    "    return any(cp.strip().zfill(5) in cp_set for cp in str(cell).split(\"|\"))\n",
    "\n",
    "mask_banlieue = banlieues_df[\"codes_postaux_banlieue\"].apply(lambda x: banlieue_a_cp_dans_liste(x, cp_set))\n",
    "extrait_banlieue = banlieues_df[mask_banlieue].copy()\n",
    "code_postal_ville.extend(extrait_banlieue[\"code_postal\"].tolist())\n",
    "dfliste = pd.Series(code_postal_ville)\n",
    "\n",
    "outfile = \"../DataCleaned/metropoleetbanlieues_liste.csv\"\n",
    "dfliste.to_csv(outfile, index=False, encoding=\"utf-8\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunks = pd.read_csv('../Data/Culture.csv', chunksize=1000000, low_memory=False, sep=\";\")\n",
    "\n",
    "\n",
    "chunk_list = []\n",
    "for chunk in chunks:\n",
    "    print(\"Processing a new chunk...\")\n",
    "    df1 = pd.DataFrame(chunk)\n",
    "    df2 = df1[['Code_Postal']].dropna()\n",
    "    chunk_list.append(df2)\n",
    "\n",
    "# Concatenate all chunks into a single DataFrame\n",
    "df = pd.concat(chunk_list, ignore_index=True)\n",
    "\n",
    "# Display basic information about the dataset\n",
    "counts = df.groupby(\"Code_Postal\").size().reset_index(name=\"CultureCount\")\n",
    "print(counts.sort_values(by=\"CultureCount\", ascending=False).head(10))\n",
    "\n",
    "counts.to_csv('../DataCleaned/Culture_Cleaned.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the CSV\n",
    "chunks = pd.read_csv('../Data/ValeurFonciere.csv', chunksize=1000000, low_memory=False, sep=\",\")\n",
    "\n",
    "chunk_list = []\n",
    "for chunk in chunks:\n",
    "    print(\"Processing a new chunk...\")\n",
    "    df1 = pd.DataFrame(chunk)\n",
    "    df1 = df1[['nature_mutation','valeur_fonciere','code_postal','surface_reelle_bati','nombre_pieces_principales','type_local','longitude','latitude','date_mutation']]\n",
    "    df1 = df1.dropna(subset=['code_postal'])\n",
    "    df1['code_postal'] = df1['code_postal'].astype(float).astype(int).astype(str).str.zfill(5)\n",
    "    df1['date_mutation'] = pd.to_datetime(df1['date_mutation'])\n",
    "    chunk_list.append(df1)\n",
    "\n",
    "# Concatenate all chunks into a single DataFrame\n",
    "dftot = pd.concat(chunk_list, ignore_index=True)\n",
    "\n",
    "# Display basic information about the dataset\n",
    "print(dftot.info())\n",
    "print(dftot.head())\n",
    "\n",
    "dftot.to_csv('../DataCleaned/DVF_Cleaned.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the CSV\n",
    "chunks = pd.read_csv('../Data/Education.csv', chunksize=1000000, low_memory=False, sep=\";\")\n",
    "\n",
    "chunk_list = []\n",
    "for chunk in chunks:\n",
    "    print(\"Processing a new chunk...\")\n",
    "    df1 = pd.DataFrame(chunk)\n",
    "    df2 = df1[['Adresse : code postal']].dropna().copy()\n",
    "    df2 = df2.rename(columns={'Adresse : code postal':'CodePostal'})\n",
    "    df2['CodePostal'] = df2['CodePostal'].apply(lambda value: str(value).zfill(5))\n",
    "    chunk_list.append(df2)\n",
    "\n",
    "# Concatenate all chunks into a single DataFrame\n",
    "df = pd.concat(chunk_list, ignore_index=True)\n",
    "count = df.groupby(\"CodePostal\").size().reset_index(name=\"EducationCount\")\n",
    "\n",
    "# Display basic information about the dataset\n",
    "print(count.info())\n",
    "print(count.head())\n",
    "print(count.sort_values(by=\"EducationCount\", ascending=False).head(10))\n",
    "count.to_csv('../DataCleaned/EducationCount_Cleaned.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the CSV\n",
    "chunks = pd.read_csv('../Data/LoyerT1T2.csv', chunksize=1000000, low_memory=False, sep=\";\", encoding=\"latin1\")\n",
    "\n",
    "chunk_list = []\n",
    "for chunk in chunks:\n",
    "    print(\"Processing a new chunk...\")\n",
    "    df1 = pd.DataFrame(chunk)\n",
    "    df2 = df1[['DEP', 'loypredm2']].dropna().copy()\n",
    "    df2 = df2.rename(columns={'DEP':'D√©partement','loypredm2':'Loyerm2'})\n",
    "    df2['Loyerm2'] = df2['Loyerm2'].astype(str).str.replace(',', '.').astype(float)\n",
    "    chunk_list.append(df2)\n",
    "\n",
    "# Concatenate all chunks into a single DataFrame\n",
    "df = pd.concat(chunk_list, ignore_index=True)\n",
    "df['Loyerm2'] = pd.to_numeric(df['Loyerm2'], errors='coerce')\n",
    "mean = df.groupby(\"D√©partement\")[\"Loyerm2\"].mean().reset_index(name=\"Loyerm2Mean\")\n",
    "\n",
    "# Display basic information about the dataset\n",
    "print(mean.sort_values(by=\"Loyerm2Mean\", ascending=False).head())\n",
    "mean.to_csv('../DataCleaned/LoyerT1T2_Cleaned.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stop_lat,stop_lon\n",
    "\n",
    "# --- Safe reference loading and normalization ---\n",
    "reference_df = pd.read_csv(\"../Data/latlongCP.csv\", sep=\",\", dtype={\"code_postal\": str}, low_memory=False)\n",
    "# Normalize postal codes and coordinates\n",
    "reference_df['code_postal'] = reference_df['code_postal'].astype(str).str.strip().str.zfill(5)\n",
    "reference_df = reference_df.dropna(subset=['latitude', 'longitude']).reset_index(drop=True)\n",
    "reference_df['lat'] = reference_df['latitude'].astype(float)\n",
    "reference_df['lon'] = reference_df['longitude'].astype(float)\n",
    "\n",
    "# Quick diagnostic: do we have 75015 in reference?\n",
    "print('\\nReference rows for 75015 (if any):')\n",
    "print(reference_df[reference_df['code_postal'] == '75015'][['code_postal','latitude','longitude','nom_commune_postal']].head())\n",
    "\n",
    "# Build BallTree (fast nearest-neighbor search) on radians\n",
    "coords_rad = np.radians(reference_df[[\"lat\", \"lon\"]].to_numpy())\n",
    "try:\n",
    "    tree = BallTree(coords_rad, metric=\"haversine\")\n",
    "except Exception as e:\n",
    "    print('Failed to build BallTree:', e)\n",
    "    raise\n",
    "\n",
    "# Batch mapping function: for many points at once, query k neighbors and accept nearest within max_km\n",
    "def latlon_to_zip_batch(lats, lons, k=3, max_km=5.0):\n",
    "    \"\"\"Map arrays of lats/lons to nearest postal code. If nearest neighbor > max_km, fallback to nearest anyway.\n",
    "    Returns list of postal codes (strings zero-padded).\n",
    "    \"\"\"\n",
    "    pts = np.column_stack([lats, lons])\n",
    "    pts = pts.astype(float)\n",
    "    pts_rad = np.radians(pts)\n",
    "    dists, idxs = tree.query(pts_rad, k=k)\n",
    "    dists_km = dists * 6371.0\n",
    "    results = []\n",
    "    for dist_row, idx_row in zip(dists_km, idxs):\n",
    "        chosen = None\n",
    "        for d, idx in zip(dist_row, idx_row):\n",
    "            if d <= max_km:\n",
    "                chosen = reference_df.iloc[idx]['code_postal']\n",
    "                break\n",
    "        if chosen is None:\n",
    "            # fallback to nearest\n",
    "            chosen = reference_df.iloc[idx_row[0]]['code_postal']\n",
    "        results.append(str(chosen).zfill(5))\n",
    "    return results\n",
    "\n",
    "# --- Process transport file in chunks (vectorized within each chunk) ---\n",
    "chunks = pd.read_csv('../Data/Transport.csv', chunksize=1000000, low_memory=False, sep=\",\")\n",
    "chunk_list = []\n",
    "processed = 0\n",
    "for chunk in chunks:\n",
    "    print(f\"Processing a new chunk (rows={len(chunk)})...\")\n",
    "    df1 = pd.DataFrame(chunk)\n",
    "    # keep only stop coords and coerce to numeric\n",
    "    if 'stop_lat' not in df1.columns or 'stop_lon' not in df1.columns:\n",
    "        print('Transport file missing stop_lat/stop_lon columns')\n",
    "        continue\n",
    "    df2 = df1[['stop_lat','stop_lon']].copy().dropna()\n",
    "    df2['stop_lat'] = pd.to_numeric(df2['stop_lat'], errors='coerce')\n",
    "    df2['stop_lon'] = pd.to_numeric(df2['stop_lon'], errors='coerce')\n",
    "    df2 = df2.dropna()\n",
    "    if df2.empty:\n",
    "        continue\n",
    "    # Batch query\n",
    "    try:\n",
    "        codes = latlon_to_zip_batch(df2['stop_lat'].to_numpy(), df2['stop_lon'].to_numpy(), k=3, max_km=5.0)\n",
    "    except Exception as e:\n",
    "        print('Error during batch mapping:', e)\n",
    "        # fallback to safe row-wise mapping\n",
    "        codes = []\n",
    "        for rlat, rlon in zip(df2['stop_lat'], df2['stop_lon']):\n",
    "            try:\n",
    "                c = latlon_to_zip_batch([rlat], [rlon], k=3, max_km=5.0)[0]\n",
    "            except Exception:\n",
    "                c = '00000'\n",
    "            codes.append(c)\n",
    "    df3 = pd.DataFrame({'CodePostal': codes})\n",
    "    chunk_list.append(df3)\n",
    "    processed += len(df2)\n",
    "    print(f\"  mapped {len(df2)} stops (total mapped so far: {processed})\")\n",
    "\n",
    "# Concatenate results\n",
    "if chunk_list:\n",
    "    transport_df = pd.concat(chunk_list, ignore_index=True)\n",
    "else:\n",
    "    transport_df = pd.DataFrame(columns=['CodePostal'])\n",
    "\n",
    "# Display counts\n",
    "transport_df['CodePostal'] = transport_df['CodePostal'].str[:2]\n",
    "counts = transport_df.groupby('CodePostal').size().reset_index(name='TransportCount')\n",
    "print('\\nTop postal codes by transport count:')\n",
    "print(counts.sort_values(by='TransportCount', ascending=False).head(20))\n",
    "\n",
    "\n",
    "# Save\n",
    "counts.to_csv('../DataCleaned/TransportCount_Cleaned.csv', index=False)\n",
    "print('\\nSaved TransportCount_Cleaned.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Les donn√©es sont nettoy√©es.\n",
    "Un version de tous chaque dataset est dissponible via le lien Google Drive suivant : https://drive.google.com/file/d/1rvZ-YiY33y1o5F8B2-NPZP1NQ5c6yshX/view?usp=sharing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data join"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all_temp = {}\n",
    "for dataset in ['Culture_Cleaned','Carburant_Cleaned','EducationCount_Cleaned','TransportCount_Cleaned','LoyerT1T2_Cleaned','metropoleetbanlieues_liste','DVF_Cleaned']:\n",
    "    df_all_temp[dataset] = pd.read_csv('DataCleaned/'+dataset+'.csv', low_memory=False, sep=\",\")\n",
    "\n",
    "df_all_temp['Culture_Cleaned']['Code_Postal'] = df_all_temp['Culture_Cleaned']['Code_Postal'].astype(str).str.zfill(5)\n",
    "df_all_temp['Carburant_Cleaned']['code_postal'] = df_all_temp['Carburant_Cleaned']['code_postal'].astype(str).str.zfill(5)\n",
    "df_all_temp['metropoleetbanlieues_liste']['0'] = df_all_temp['metropoleetbanlieues_liste']['0'].astype(str).str.zfill(5)\n",
    "df_all_temp['EducationCount_Cleaned']['CodePostal'] = df_all_temp['EducationCount_Cleaned']['CodePostal'].astype(str).str.zfill(5)\n",
    "df_all_temp['DVF_Cleaned']['code_postal'] = df_all_temp['DVF_Cleaned']['code_postal'].astype(str).str.zfill(5)\n",
    "\n",
    "df_all = df_all_temp['Culture_Cleaned'].merge(df_all_temp['Carburant_Cleaned'], left_on=\"Code_Postal\", right_on=\"code_postal\")\n",
    "df_all_temp.pop('Culture_Cleaned')\n",
    "df_all_temp.pop('Carburant_Cleaned')\n",
    "df_all = df_all.merge(df_all_temp['metropoleetbanlieues_liste'], left_on=\"Code_Postal\", right_on=\"0\", how='left')\n",
    "df_all_temp.pop('metropoleetbanlieues_liste')\n",
    "df_all = df_all.merge(df_all_temp['EducationCount_Cleaned'], left_on=\"Code_Postal\", right_on=\"CodePostal\", how='left')\n",
    "df_all_temp.pop('EducationCount_Cleaned')\n",
    "df_all['D√©partement'] = df_all['CodePostal'].apply(lambda x : str(x).zfill(5)).str[:2]\n",
    "df_all_temp['TransportCount_Cleaned'] = df_all_temp['TransportCount_Cleaned'].astype(str)\n",
    "df_all = df_all.merge(df_all_temp['TransportCount_Cleaned'], left_on=\"D√©partement\", right_on=\"CodePostal\", how='left')\n",
    "df_all_temp.pop('TransportCount_Cleaned')\n",
    "df_all = df_all.merge(df_all_temp['LoyerT1T2_Cleaned'], left_on=\"D√©partement\", right_on=\"D√©partement\", how='left')\n",
    "df_all_temp.pop('LoyerT1T2_Cleaned')\n",
    "df_all = df_all.merge(df_all_temp['DVF_Cleaned'], left_on=\"Code_Postal\", right_on=\"code_postal\", how='left')\n",
    "df_all_temp.pop('DVF_Cleaned')\n",
    "df_all['CodePostal'] = df_all['Code_Postal'].apply(lambda value: str(value).zfill(5))\n",
    "\n",
    "\n",
    "df_all.to_csv('DataAll/Dataset.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          Code_Postal    CultureCount   code_postal_x     prix_gazole  \\\n",
      "count  1000000.000000  1000000.000000  1000000.000000  1000000.000000   \n",
      "mean      4364.282960       35.596975     4364.282960        1.631407   \n",
      "std       2242.071663       49.078891     2242.071663        0.054701   \n",
      "min       1000.000000        1.000000     1000.000000        1.535000   \n",
      "25%       2200.000000       11.000000     2200.000000        1.589000   \n",
      "50%       4400.000000       19.000000     4400.000000        1.617286   \n",
      "75%       6340.000000       41.000000     6340.000000        1.666000   \n",
      "max       8090.000000      257.000000     8090.000000        1.890000   \n",
      "\n",
      "           prix_sp95       prix_e85      prix_gplc      prix_sp98  \\\n",
      "count  587454.000000  683089.000000  332507.000000  948811.000000   \n",
      "mean        1.743556       0.725266       0.982607       1.794995   \n",
      "std         0.061140       0.058205       0.049130       0.058433   \n",
      "min         1.649000       0.639000       0.834000       1.687000   \n",
      "25%         1.700000       0.681500       0.955000       1.750000   \n",
      "50%         1.731500       0.709000       0.979000       1.776857   \n",
      "75%         1.760000       0.745000       1.014000       1.831500   \n",
      "max         1.970000       1.010000       1.089000       2.019500   \n",
      "\n",
      "            prix_e10         totmean  ...  TransportCount     D√©partement  \\\n",
      "count  887687.000000  1000000.000000  ...    35841.000000  1000000.000000   \n",
      "mean        1.695990        1.563500  ...     8439.953852        4.065323   \n",
      "std         0.055412        0.125546  ...     7552.770380        2.256376   \n",
      "min         1.598000        1.275333  ...      159.000000        1.000000   \n",
      "25%         1.651500        1.479000  ...      935.000000        2.000000   \n",
      "50%         1.682714        1.510200  ...    10330.000000        4.000000   \n",
      "75%         1.739000        1.668083  ...    16053.000000        6.000000   \n",
      "max         1.980000        1.912333  ...    16053.000000        8.000000   \n",
      "\n",
      "          Loyerm2Mean  valeur_fonciere   code_postal_y  surface_reelle_bati  \\\n",
      "count  1000000.000000     9.948000e+05  1000000.000000        331612.000000   \n",
      "mean        12.048851     4.068190e+05     4364.282960           112.890610   \n",
      "std          2.080292     2.783772e+06     2242.071663           831.581877   \n",
      "min          9.060860     1.500000e-01     1000.000000             1.000000   \n",
      "25%         10.167553     5.980000e+04     2200.000000            47.000000   \n",
      "50%         11.546030     1.540000e+05     4400.000000            72.000000   \n",
      "75%         15.057766     3.000000e+05     6340.000000           104.000000   \n",
      "max         15.057766     2.206233e+08     8090.000000        265000.000000   \n",
      "\n",
      "       nombre_pieces_principales      longitude       latitude      CodePostal  \n",
      "count              596409.000000  983023.000000  983023.000000  1000000.000000  \n",
      "mean                    1.668778       5.336644      45.584257     4364.282960  \n",
      "std                     2.014438       1.495258       1.940893     2242.071663  \n",
      "min                     0.000000       2.303984      43.518055     1000.000000  \n",
      "25%                     0.000000       4.054221      43.777342     2200.000000  \n",
      "50%                     1.000000       5.324396      45.054258     4400.000000  \n",
      "75%                     3.000000       6.936436      46.297228     6340.000000  \n",
      "max                    53.000000       7.701981      50.067463     8090.000000  \n",
      "\n",
      "[8 rows x 24 columns]\n",
      "Dimensions: (0, 0)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: []\n",
       "Index: []"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "chunks = pd.read_csv(\"DataAll/Dataset.csv\", chunksize=1000000, low_memory=False, sep=\",\")\n",
    "\n",
    "chunk_list = pd.DataFrame()\n",
    "for chunk in chunks:\n",
    "    print(chunk.describe())\n",
    "    break\n",
    "\n",
    "\n",
    "print(\"Dimensions:\", chunk_list.shape)\n",
    "display(chunk_list.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Une fois les donn√©es rassembl√©e, nous pouvons les utiliser pour faire l'analyse.\n",
    "Une version de ce dataset join est disponible via le lien Google Drive suivant : https://drive.google.com/file/d/1T7yU4MXeBOWTg1t0p1GGr0h-_UjZtCwr/view?usp=sharing"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venvMacOS",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
